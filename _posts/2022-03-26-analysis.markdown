---
layout: post
title:  "Analysis on Learnt Represenatation"
date:   2022-03-26 12:04:39 +0530
categories: jekyll update
permalink: representation_analysis
image_our: /assets/img/distribution_timit_ours.png
image_pre: /assets/img/distribution_timit_pretrained.png
image_plot_our: /assets/img/plot_timit_ours.png
image_plot_pre: /assets/img/plot_timit_pre.png
image_statistical: /assets/img/kws_significance_test.png
ds: /assets/img/tamil_words.txt
---
# __Tamil Script Transliteration:__

The transliteration of the tamil keywords from MSWC dataset for 190 keywords is attached here: [Text file]({{ page.ds | relative_url}})

# __Low Resource Language Evaluation:__

## __Vallader:__


The total number of words used for training is 233. The IV words used for evaluating results is 66 and OOV words used to evaluate the results is 59
The total number of IV and OOV pairs is 0.5 million. The number of IV pairs is 0.1 million. The number of OOV pairs is 0.1 million. 

| Model | ALL | IV | OOV |
|-------|-----|----|-----|
|Pretrained + Triplet | 0.383 | 0.545 | 0.566 |
|CTC + Triplet | 0.313 | 0.450 | 0.536 |
|Pretrained + CTC + Triplet | 0.383 | 0.545 | 0.566 |

## __Hausa:__

The total number of words used for training is 240. The total number of IV and OOV used to evaluate the performance is 77 and 61 words.
The total number of IV and OOV pairs is 1.8 million. The number of IV pairs is 0.3 million. The number of OOV pairs is 0.5 million.

| Model | ALL | IV | OOV |
|-------|-----|----|-----|
|Pretrained + Triplet | 0.082 | 0.135 | 0.187 |
|CTC + Triplet | 0.408 | 0.552 | 0.604 |
|Pretrained + CTC + Triplet | 0.428 | 0.554 | 0.634 |

# __Statistical Significance Test:__

Here we show Critical difference (CD) diagram using Friedman Test and a post-hoc pair-wise Nemenyi test, with a significance level of 0.05.  The higher the rank, the better the technique. The x axis where the lines end represents the average rank position of the respective methods for TIMIT datasets. The null hypothesis is that the average ranks of each pair of methods do not differ with statistical significance. Horizontal lines connect the lines of the methods for which we cannot exclude the hypothesis that their average ranks are equal. Any pair of methods whose lines are not connected with a horizontal line can be seen as having an average rank that is different with statistical significance. Our Pretrained + CTC + Triplet method's rank is higher than all other methods. Hence our method is statistically significant over all other methods.  The CD is 0.09.

Statisitcal Significance Test |
:--------: |
![]({{ page.image_statistical | relative_url }}) |



# __Distribution of Distances between Positive and Negative pairs:__

Distribution of distances between word embeddings within positive and negative paris.  Positive pair means that both samples are drawn form the same word. Negative pair assumes that samples from this pair are drawn from different classes.
The results are shown on TIMIT dataset.

Pretrained             |  Ours
:-------------------------:|:-------------------------:
![]({{ page.image_pre | relative_url }})  |  ![]({{ page.image_our | relative_url }})


It can be observed that the positive and negative paris are well separated in our model compared with the pretrained model.

# __t-SNE Plot of word embeddings on TIMIT test set:__

t-SNE Pretrained |
:--------: |
![]({{ page.image_plot_pre | relative_url }}) |

t-SNE Our Triplet + CTC + Pretrained |
:--------: |
![]({{ page.image_plot_our | relative_url }}) |


It can be observed that similar words are better seperated from other words in our model compared to the pretrained model.
